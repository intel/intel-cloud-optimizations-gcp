# PIPELINE DEFINITION
# Name: intel-xgboost-daal4py-pipeline
# Description: XGBoost-Daal4py Pipeline
# Inputs:
#    data_size: int [Default: 1000000.0]
#    data_url: str
# Outputs:
#    daal4py-inference-metrics: system.Metrics
#    plot-roc-curve-class_metrics: system.ClassificationMetrics
components:
  comp-convert-xgboost-to-daal4py:
    executorLabel: exec-convert-xgboost-to-daal4py
    inputDefinitions:
      artifacts:
        xgb_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        daal4py_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-create-train-test-set:
    executorLabel: exec-create-train-test-set
    inputDefinitions:
      artifacts:
        data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        x_test_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        x_train_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_test_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_train_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-daal4py-inference:
    executorLabel: exec-daal4py-inference
    inputDefinitions:
      artifacts:
        daal4py_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        x_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        prediction_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        report:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-load-data:
    executorLabel: exec-load-data
    inputDefinitions:
      parameters:
        data_size:
          parameterType: NUMBER_INTEGER
        data_url:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        credit_risk_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-plot-roc-curve:
    executorLabel: exec-plot-roc-curve
    inputDefinitions:
      artifacts:
        predictions:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        class_metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
  comp-preprocess-features:
    executorLabel: exec-preprocess-features
    inputDefinitions:
      artifacts:
        x_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        x_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        x_test_processed:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        x_train_processed:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-xgboost-model:
    executorLabel: exec-train-xgboost-model
    inputDefinitions:
      artifacts:
        x_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        xgb_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-convert-xgboost-to-daal4py:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - convert_xgboost_to_daal4py
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'daal4py' 'joblib'\
          \ 'loguru' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef convert_xgboost_to_daal4py(\n    xgb_model: Input[Model],\n \
          \   daal4py_model: Output[Model]\n):\n\n    \"\"\"\n    Converts XGBoost\
          \ model to inference-optimized daal4py classifier.\n\n    Input Artifacts\n\
          \    ---------------\n    xgb_model : Model\n        trained XGBoost classifier\
          \ \n\n    Output Artifacts\n    ----------------\n    daal4py_model : Model\n\
          \        inference-optimized daal4py classifier  \n    \"\"\"\n\n    import\
          \ daal4py as d4p\n    import joblib\n    from loguru import logger\n\n \
          \   with open(xgb_model.path, \"rb\") as file_reader:\n        clf = joblib.load(file_reader)\n\
          \n    logger.info(\"Converting XGBoost model to Daal4py...\")\n    daal_model\
          \ = d4p.get_gbt_model_from_xgboost(clf)\n    logger.info(\"Done!\")\n\n\
          \    with open(daal4py_model.path, \"wb\") as file_writer:\n        joblib.dump(daal_model,\
          \ file_writer)\n\n"
        image: python:3.10
    exec-create-train-test-set:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - create_train_test_set
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ 'loguru' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef create_train_test_set(\n    data: Input[Dataset],\n    x_train_data:\
          \ Output[Dataset],\n    y_train_data: Output[Dataset],\n    x_test_data:\
          \ Output[Dataset],\n    y_test_data: Output[Dataset]\n):\n\n    \"\"\"\n\
          \    Creates 75:25 split of input dataset for model evaluation.\n\n    Input\
          \ Artifacts\n    ---------------\n    data : Dataset\n        dataset that\
          \ has been synthetically augmented by the load_data() function\n\n    Output\
          \ Artifacts\n    ----------------\n    x_train_data : Dataset\n        training\
          \ features, 75% of original dataset\n    y_train_data : Dataset\n      \
          \  training labels of target variable, loan_status \n    x_test_data : Dataset\n\
          \        test features, 25% of original dataset\n    y_test_data : Dataset\n\
          \        test labels of target variable, loan_status\n    \"\"\" \n\n  \
          \  import pandas as pd\n    from loguru import logger\n    from sklearn.model_selection\
          \ import train_test_split\n\n    data = pd.read_csv(data.path)\n\n    logger.info(\"\
          Creating training and test sets...\")\n    train, test = train_test_split(data,\
          \ test_size = 0.25, random_state = 0)\n\n    X_train = train.drop([\"loan_status\"\
          ], axis = 1)\n    y_train = train[\"loan_status\"]\n\n    X_test = test.drop([\"\
          loan_status\"], axis = 1)\n    y_test = test[\"loan_status\"]\n\n    logger.info(\"\
          Training and test sets created.\\n\" \\\n                \"X_train size:\
          \ {}, y_train size: {}\\n\" \\\n                \"X_test size: {}, y_test\
          \ size: {}\", \n                X_train.shape, y_train.shape, X_test.shape,\
          \ y_test.shape)\n\n    X_train.to_csv(x_train_data.path, index = False)\n\
          \    y_train.to_csv(y_train_data.path, index = False, header = None)\n \
          \   X_test.to_csv(x_test_data.path, index = False)\n    y_test.to_csv(y_test_data.path,\
          \ index = False, header = None)\n\n"
        image: python:3.10
    exec-daal4py-inference:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - daal4py_inference
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'daal4py' 'pandas'\
          \ 'scikit-learn' 'scikit-learn-intelex' 'joblib' 'loguru' && \"$0\" \"$@\"\
          \n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef daal4py_inference(\n    x_test: Input[Dataset],\n    y_test:\
          \ Input[Dataset],\n    daal4py_model: Input[Model],\n    prediction_data:\
          \ Output[Dataset],\n    report: Output[Dataset],\n    metrics: Output[Metrics]\n\
          ):\n\n    \"\"\"\n    Computes predictions using the inference-optimized\
          \ daal4py classifier \n    and evaluates model performance.\n\n    Input\
          \ Artifacts\n    ---------------\n    x_test : Dataset\n        processed\
          \ and scaled test features \n    y_test : Dataset\n        test labels of\
          \ target variable, loan_status \n    daal4py_model : Model\n        inference-optimized\
          \ daal4py classifier\n\n    Output Artifacts\n    ----------------\n   \
          \ prediction_data : Dataset\n        dataset containing true test labels\
          \ and predicted probabilities\n    report : Dataset\n        summary of\
          \ the precision, recall, F1 score for each class\n    metrics : Metrics\n\
          \        scalar classification metrics containing the model's AUC and accuracy\n\
          \    \"\"\"\n\n    import daal4py as d4p\n    import joblib\n    import\
          \ pandas as pd\n\n    from sklearnex import patch_sklearn\n    patch_sklearn()\n\
          \    from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n\
          \n    X_test = pd.read_csv(x_test.path, header = None)\n    y_test = pd.read_csv(y_test.path,\
          \ header = None)\n\n    with open(daal4py_model.path, \"rb\") as file_reader:\n\
          \        daal_model = joblib.load(file_reader)\n\n    daal_prediction =\
          \ d4p.gbt_classification_prediction(\n            nClasses = 2, \n     \
          \       resultsToEvaluate = \"computeClassLabels|computeClassProbabilities\"\
          \n        ).compute(X_test, daal_model)\n\n    y_pred = daal_prediction.prediction\n\
          \    y_prob = daal_prediction.probabilities[:,1]\n\n    results = classification_report(\n\
          \        y_test, y_pred,\n        target_names = [\"Non-Default\", \"Default\"\
          ],\n        output_dict = True\n    )\n    results = pd.DataFrame(results).transpose()\n\
          \    results.to_csv(report.path)\n\n    auc = roc_auc_score(y_test, y_prob)\n\
          \    metrics.log_metric('AUC', auc)\n\n    accuracy = (accuracy_score(y_test,\
          \ y_pred)*100)\n    metrics.log_metric('Accuracy', accuracy)\n\n    predictions\
          \ = pd.DataFrame({'y_test': y_test.values.flatten(), \n                \
          \                'y_prob': y_prob})\n    predictions.to_csv(prediction_data.path,\
          \ index = False)\n\n"
        image: python:3.10
    exec-load-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy' 'pandas'\
          \ 'loguru' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_data(\n    data_url: str,\n    data_size: int, \n    credit_risk_dataset:\
          \ Output[Dataset]\n):\n\n    \"\"\"\n    Downloads credit_risk_dataset.csv\
          \ file and generates  \n    additional synthetic data for benchmarking and\
          \ testing purposes.\n\n    Input Parameters\n    ----------------\n    data_url\
          \ : str\n        url where the dataset is hosted\n    data_size : int\n\
          \        size of final dataset desired, default 1M rows\n\n    Output Artifacts\n\
          \    ----------------\n    credit_risk_dataset : Dataset\n        data that\
          \ has been synthetically augmented or loaded from URL provided\n    \"\"\
          \" \n\n    import numpy as np\n    import pandas as pd\n    from loguru\
          \ import logger\n\n    logger.info(\"Loading csv from {}...\", data_url)\n\
          \    data = pd.read_csv(data_url)\n    logger.info(\"Done!\")\n\n    # number\
          \ of rows to generate\n    if data_size < data.shape[0]:\n        pass\n\
          \    else:\n        logger.info(\"Generating {:,} rows of data...\", data_size)\n\
          \        repeats = data_size // len(data)\n        data = data.loc[np.repeat(data.index.values,\
          \ repeats + 1)]\n        data = data.iloc[:data_size]\n\n        # perturbing\
          \ all int/float columns\n        person_age = data[\"person_age\"].values\
          \ + np.random.randint(\n            -1, 1, size=len(data)\n        )\n \
          \       person_income = data[\"person_income\"].values + np.random.normal(\n\
          \            0, 10, size=len(data)\n        )\n        person_emp_length\
          \ = data[\n            \"person_emp_length\"\n        ].values + np.random.randint(-1,\
          \ 1, size=len(data))\n        loan_amnt = data[\"loan_amnt\"].values + np.random.normal(\n\
          \            0, 5, size=len(data)\n        )\n        loan_int_rate = data[\"\
          loan_int_rate\"].values + np.random.normal(\n            0, 0.2, size=len(data)\n\
          \        )\n        loan_percent_income = data[\"loan_percent_income\"].values\
          \ + (\n            np.random.randint(0, 100, size=len(data)) / 1000\n  \
          \      )\n        cb_person_cred_hist_length = data[\n            \"cb_person_cred_hist_length\"\
          \n        ].values + np.random.randint(0, 2, size=len(data))\n\n       \
          \ # perturbing all binary columns\n        perturb_idx = np.random.rand(len(data))\
          \ > 0.1\n        random_values = np.random.choice(\n            data[\"\
          person_home_ownership\"].unique(), len(data)\n        )\n        person_home_ownership\
          \ = np.where(\n            perturb_idx, data[\"person_home_ownership\"],\
          \ random_values\n        )\n        perturb_idx = np.random.rand(len(data))\
          \ > 0.1\n        random_values = np.random.choice(\n            data[\"\
          loan_intent\"].unique(), len(data)\n        )\n        loan_intent = np.where(perturb_idx,\
          \ data[\"loan_intent\"], random_values)\n        perturb_idx = np.random.rand(len(data))\
          \ > 0.1\n        random_values = np.random.choice(\n            data[\"\
          loan_grade\"].unique(), len(data)\n        )\n        loan_grade = np.where(perturb_idx,\
          \ data[\"loan_grade\"], random_values)\n        perturb_idx = np.random.rand(len(data))\
          \ > 0.1\n        random_values = np.random.choice(\n            data[\"\
          cb_person_default_on_file\"].unique(), len(data)\n        )\n        cb_person_default_on_file\
          \ = np.where(\n            perturb_idx, data[\"cb_person_default_on_file\"\
          ], random_values\n        )\n        data = pd.DataFrame(\n            list(\n\
          \                zip(\n                    person_age,\n               \
          \     person_income,\n                    person_home_ownership,\n     \
          \               person_emp_length,\n                    loan_intent,\n \
          \                   loan_grade,\n                    loan_amnt,\n      \
          \              loan_int_rate,\n                    data[\"loan_status\"\
          ].values,\n                    loan_percent_income,\n                  \
          \  cb_person_default_on_file,\n                    cb_person_cred_hist_length,\n\
          \                )\n            ),\n            columns = data.columns,\n\
          \        )\n\n        data = data.drop_duplicates()\n        assert len(data)\
          \ == data_size\n        data.reset_index(drop = True)\n\n    data.to_csv(credit_risk_dataset.path,\
          \ index = None)\n\n"
        image: python:3.10
    exec-plot-roc-curve:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - plot_roc_curve
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ 'scikit-learn-intelex' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef plot_roc_curve(\n    predictions: Input[Dataset],\n    class_metrics:\
          \ Output[ClassificationMetrics]\n):\n\n    \"\"\"\n    Function to plot\
          \ Receiver Operating Characteristic (ROC) curve.\n\n    Input Artifacts\n\
          \    ---------------\n    predictions : Dataset\n        dataset containing\
          \ true test labels and predicted probabilities\n\n    Output Artifacts\n\
          \    ----------------\n    class_metrics : ClassificationMetrics\n     \
          \   classification metrics containing fpr, tpr, and thresholds \n    \"\"\
          \"\n\n    import pandas as pd\n\n    from sklearnex import patch_sklearn\n\
          \    patch_sklearn()\n    from sklearn.metrics import roc_curve\n\n    prediction_data\
          \ = pd.read_csv(predictions.path)\n\n    fpr, tpr, thresholds = roc_curve(\n\
          \        y_true = prediction_data['y_test'], \n        y_score = prediction_data['y_prob'],\
          \ \n        pos_label = 1)\n\n    class_metrics.log_roc_curve(fpr, tpr,\
          \ thresholds)\n\n"
        image: python:3.10
    exec-preprocess-features:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess_features
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess_features(\n    x_train: Input[Dataset],\n    x_test:\
          \ Input[Dataset],\n    x_train_processed: Output[Dataset],\n    x_test_processed:\
          \ Output[Dataset]\n):\n\n    \"\"\"\n    Performs data preprocessing of\
          \ training and test features.\n\n    Input Artifacts\n    ---------------\n\
          \    x_train : Dataset\n        original unprocessed training features \n\
          \    x_test : Dataset\n        original unprocessed test features\n\n  \
          \  Output Artifacts\n    ----------------\n    x_train_processed : Dataset\n\
          \        processed and scaled training features\n    x_test_processed :\
          \ Dataset\n        processed and scaled test features  \n    \"\"\" \n\n\
          \    import pandas as pd    \n    from sklearn.compose import ColumnTransformer\n\
          \    from sklearn.impute import SimpleImputer\n    from sklearn.pipeline\
          \ import Pipeline\n    from sklearn.preprocessing import OneHotEncoder,\
          \ PowerTransformer\n\n    X_train = pd.read_csv(x_train.path)\n    X_test\
          \ = pd.read_csv(x_test.path)\n\n    # data processing pipeline\n    num_imputer\
          \ = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy = \"median\"))])\n\
          \    pow_transformer = PowerTransformer()\n    cat_transformer = OneHotEncoder(handle_unknown\
          \ = \"ignore\")\n    preprocessor = ColumnTransformer(\n        transformers\
          \ = [\n            (\n                \"num\",\n                num_imputer,\n\
          \                [\n                    \"loan_int_rate\",\n           \
          \         \"person_emp_length\",\n                    \"cb_person_cred_hist_length\"\
          ,\n                ],\n            ),\n            (\n                \"\
          pow\",\n                pow_transformer,\n                [\"person_age\"\
          , \"person_income\", \"loan_amnt\", \"loan_percent_income\"],\n        \
          \    ),\n            (\n                \"cat\",\n                cat_transformer,\n\
          \                [\n                    \"person_home_ownership\",\n   \
          \                 \"loan_intent\",\n                    \"loan_grade\",\n\
          \                    \"cb_person_default_on_file\",\n                ],\n\
          \            ),\n        ],\n        remainder=\"passthrough\",\n    )\n\
          \n    preprocess = Pipeline(steps = [(\"preprocessor\", preprocessor)])\n\
          \n    X_train = pd.DataFrame(preprocess.fit_transform(X_train))\n    X_test\
          \ = pd.DataFrame(preprocess.transform(X_test))\n\n    X_train.to_csv(x_train_processed.path,\
          \ index = False, header = None)\n    X_test.to_csv(x_test_processed.path,\
          \ index = False, header = None)\n\n"
        image: python:3.10
    exec-train-xgboost-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_xgboost_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'xgboost'\
          \ 'joblib' 'loguru' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_xgboost_model(\n    x_train: Input[Dataset],\n    y_train:\
          \ Input[Dataset],\n    xgb_model: Output[Model]\n):\n\n    \"\"\"\n    Trains\
          \ an XGBoost classification model.\n\n    Input Artifacts\n    ---------------\n\
          \    x_train : Dataset\n        processed and scaled training features \n\
          \    y_train : Dataset\n        training labels of target variable, loan_status\n\
          \n    Output Artifacts\n    ----------------\n    xgb_model : Model\n  \
          \      trained XGBoost model  \n    \"\"\"\n\n    import joblib\n    import\
          \ pandas as pd\n    import xgboost as xgb\n    from loguru import logger\n\
          \n    X_train = pd.read_csv(x_train.path, header = None)\n    y_train =\
          \ pd.read_csv(y_train.path, header = None)\n\n    dtrain = xgb.DMatrix(X_train.values,\
          \ y_train.values)\n\n    params = {\n        \"objective\": \"binary:logistic\"\
          ,\n        \"eval_metric\": \"logloss\",\n        \"nthread\": 4,  # num_cpu\n\
          \        \"tree_method\": \"hist\",\n        \"learning_rate\": 0.02,\n\
          \        \"max_depth\": 10,\n        \"min_child_weight\": 6,\n        \"\
          n_jobs\": 4,  # num_cpu,\n        \"verbosity\": 1\n    }\n\n    # train\
          \ XGBoost model\n    logger.info(\"Training XGBoost model...\")\n    clf\
          \ = xgb.train(params = params, \n                    dtrain = dtrain, \n\
          \                    num_boost_round = 500)\n\n    with open(xgb_model.path,\
          \ \"wb\") as file_writer:\n        joblib.dump(clf, file_writer) \n\n"
        image: python:3.10
pipelineInfo:
  description: XGBoost-Daal4py Pipeline
  name: intel-xgboost-daal4py-pipeline
root:
  dag:
    outputs:
      artifacts:
        daal4py-inference-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: daal4py-inference
        plot-roc-curve-class_metrics:
          artifactSelectors:
          - outputArtifactKey: class_metrics
            producerSubtask: plot-roc-curve
    tasks:
      convert-xgboost-to-daal4py:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-convert-xgboost-to-daal4py
        dependentTasks:
        - train-xgboost-model
        inputs:
          artifacts:
            xgb_model:
              taskOutputArtifact:
                outputArtifactKey: xgb_model
                producerTask: train-xgboost-model
        taskInfo:
          name: Convert XGBoost model to Daal4py
      create-train-test-set:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-create-train-test-set
        dependentTasks:
        - load-data
        inputs:
          artifacts:
            data:
              taskOutputArtifact:
                outputArtifactKey: credit_risk_dataset
                producerTask: load-data
        taskInfo:
          name: Create training and test sets
      daal4py-inference:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-daal4py-inference
        dependentTasks:
        - convert-xgboost-to-daal4py
        - create-train-test-set
        - preprocess-features
        inputs:
          artifacts:
            daal4py_model:
              taskOutputArtifact:
                outputArtifactKey: daal4py_model
                producerTask: convert-xgboost-to-daal4py
            x_test:
              taskOutputArtifact:
                outputArtifactKey: x_test_processed
                producerTask: preprocess-features
            y_test:
              taskOutputArtifact:
                outputArtifactKey: y_test_data
                producerTask: create-train-test-set
        taskInfo:
          name: Daal4py Inference
      load-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-load-data
        inputs:
          parameters:
            data_size:
              componentInputParameter: data_size
            data_url:
              componentInputParameter: data_url
        taskInfo:
          name: Load data
      plot-roc-curve:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-plot-roc-curve
        dependentTasks:
        - daal4py-inference
        inputs:
          artifacts:
            predictions:
              taskOutputArtifact:
                outputArtifactKey: prediction_data
                producerTask: daal4py-inference
        taskInfo:
          name: Plot ROC Curve
      preprocess-features:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-preprocess-features
        dependentTasks:
        - create-train-test-set
        inputs:
          artifacts:
            x_test:
              taskOutputArtifact:
                outputArtifactKey: x_test_data
                producerTask: create-train-test-set
            x_train:
              taskOutputArtifact:
                outputArtifactKey: x_train_data
                producerTask: create-train-test-set
        taskInfo:
          name: Preprocess features
      train-xgboost-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-xgboost-model
        dependentTasks:
        - create-train-test-set
        - preprocess-features
        inputs:
          artifacts:
            x_train:
              taskOutputArtifact:
                outputArtifactKey: x_train_processed
                producerTask: preprocess-features
            y_train:
              taskOutputArtifact:
                outputArtifactKey: y_train_data
                producerTask: create-train-test-set
        taskInfo:
          name: Train XGBoost model
  inputDefinitions:
    parameters:
      data_size:
        defaultValue: 1000000.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      data_url:
        parameterType: STRING
  outputDefinitions:
    artifacts:
      daal4py-inference-metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      plot-roc-curve-class_metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.4.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-convert-xgboost-to-daal4py:
          nodeSelector:
            labels:
              beta.kubernetes.io/instance-type: c3-highcpu-4
        exec-create-train-test-set:
          nodeSelector:
            labels:
              beta.kubernetes.io/instance-type: c3-highcpu-4
        exec-daal4py-inference:
          nodeSelector:
            labels:
              beta.kubernetes.io/instance-type: c3-highcpu-4
        exec-load-data:
          nodeSelector:
            labels:
              beta.kubernetes.io/instance-type: c3-highcpu-4
        exec-plot-roc-curve:
          nodeSelector:
            labels:
              beta.kubernetes.io/instance-type: c3-highcpu-4
        exec-preprocess-features:
          nodeSelector:
            labels:
              beta.kubernetes.io/instance-type: c3-highcpu-4
        exec-train-xgboost-model:
          nodeSelector:
            labels:
              beta.kubernetes.io/instance-type: c3-highcpu-4
